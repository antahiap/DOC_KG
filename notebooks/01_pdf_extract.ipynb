{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pdf_file = Path(\"..\\\\data\\\\article_pdf\\\\1706.03762.pdf\")\n",
    "assert pdf_file.is_file()\n",
    "\n",
    "file_name = \"https://arxiv.org/pdf/2201.04234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = 'vya1TxPJKsBtZrAidZ66Udt7gyFfsSgs'\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "ocr_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": file_name\n",
    "    },\n",
    "    include_image_base64=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def serialize(obj):\n",
    "    return obj.__dict__ if hasattr(obj, '__dict__') else str(obj)\n",
    "\n",
    "# Convert to JSON with a custom serializer\n",
    "ocr_json = json.dumps(ocr_response, default=serialize, indent=4)\n",
    "\n",
    "# Save to a file\n",
    "with open(\"output.json\", \"w\") as file:\n",
    "    file.write(ocr_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract images\n",
    "images = [y['image_base64'] for x in data[\"pages\"] if  x.get('images') for y in x['images']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "import base64\n",
    "\n",
    "\n",
    "# Base64 string of the image (example)\n",
    "image_base64 = images[4]\n",
    "\n",
    "\n",
    "# If there's a header, remove it\n",
    "if image_base64.startswith(\"data:image\"):\n",
    "    image_base64 = image_base64.split(\",\")[1]\n",
    "\n",
    "# Decode and display\n",
    "try:\n",
    "    display(Image(data=base64.b64decode(image_base64)))\n",
    "except Exception as e:\n",
    "    print(f\"Failed to display image: {e}\")# Display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marckdown to Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# LEVERAGING UNLABELED DATA TO PREDICT OUT-OF-DISTRIBUTION PERFORMANCE \\n\\nSaurabh Garg*<br>Carnegie Mellon University<br>sgarg2@andrew.cmu.edu<br>Sivaraman Balakrishnan<br>Carnegie Mellon University<br>sbalakri@andrew.cmu.edu<br>Zachary C. Lipton<br>Carnegie Mellon University<br>zlipton@andrew.cmu.edu\\n\\n## Behnam Neyshabur\\n\\nGoogle Research, Blueshift team\\nneyshabur@google.com\\n\\nHanie Sedghi<br>Google Research, Brain team<br>hsedghi@google.com\\n\\n\\n#### Abstract\\n\\nReal-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance $2-4 \\\\times$ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works ${ }^{1}$.\\n\\n\\n## 1 INTRODUCTION\\n\\nMachine learning models deployed in the real world typically encounter examples from previously unseen distributions. While the IID assumption enables us to evaluate models using held-out data from the source distribution (from which training data is sampled), this estimate is no longer valid in presence of a distribution shift. Moreover, under such shifts, model accuracy tends to degrade (Szegedy et al., 2014; Recht et al., 2019; Koh et al., 2021). Commonly, the only data available to the practitioner are a labeled training set (source) and unlabeled deployment-time data which makes the problem more difficult. In this setting, detecting shifts in the distribution of covariates is known to be possible (but difficult) in theory (Ramdas et al., 2015), and in practice (Rabanser et al., 2018). However, producing an optimal predictor using only labeled source and unlabeled target data is well-known to be impossible absent further assumptions (Ben-David et al., 2010; Lipton et al., 2018).\\n\\nTwo vital questions that remain are: (i) the precise conditions under which we can estimate a classifier's target-domain accuracy; and (ii) which methods are most practically useful. To begin, the straightforward way to assess the performance of a model under distribution shift would be to collect labeled (target domain) examples and then to evaluate the model on that data. However, collecting fresh labeled data from the target distribution is prohibitively expensive and time-consuming, especially if the target distribution is non-stationary. Hence, instead of using labeled data, we aim to use unlabeled data from the target distribution, that is comparatively abundant, to predict model performance. Note that in this work, our focus is not to improve performance on the target but, rather, to estimate the accuracy on the target for a given classifier.\\n\\n[^0]\\n[^0]:    * Work done in part while Saurabh Garg was interning at Google\\n    ${ }^{1}$ Code is available at https://github.com/saurabhgarg1996/ATC_code.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = [x['markdown'] for x in data['pages']]\n",
    "md[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": null,\n",
      "    \"headers\": [\n",
      "        {\n",
      "            \"level\": 3,\n",
      "            \"text\": \"5.1 RESULTS\"\n",
      "        }\n",
      "    ],\n",
      "    \"paragraphs\": [\n",
      "        \"| Dataset | Shift | IM |  | AC |  | DOC |  | GDE | ATC-MC (Ours) |  | ATC-NE (Ours) |  |\\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\\n|  |  | Pre T | Post T | Pre T | Post T | Pre T | Post T | Post T | Pre T | Post T | Pre T | Post T |\\n| CIFAR10 | Natural | 6.60 | 5.74 | 9.88 | 6.89 | 7.25 | 6.07 | 4.77 | 3.21 | 3.02 | 2.99 | 2.85 |\\n|  | Synthetic | 12.33 | 10.20 | 16.50 | 11.91 | 13.87 | 11.08 | 6.55 | 4.65 | 4.25 | 4.21 | 3.87 |\\n| CIFAR100 | Synthetic | 13.69 | 11.51 | 23.61 | 13.10 | 14.60 | 10.14 | 9.85 | 5.50 | 4.75 | 4.72 | 4.94 |\\n| ImageNet200 | Natural | 12.37 | 8.19 | 22.07 | 8.61 | 15.17 | 7.81 | 5.13 | 4.37 | 2.04 | 3.79 | 1.45 |\\n|  | Synthetic | 19.86 | 12.94 | 32.44 | 13.35 | 25.02 | 12.38 | 5.41 | 5.93 | 3.09 | 5.00 | 2.68 |\\n| ImageNet | Natural | 7.77 | 6.50 | 18.13 | 6.02 | 8.13 | 5.76 | 6.23 | 3.88 | 2.17 | 2.06 | 0.80 |\\n|  | Synthetic | 13.39 | 10.12 | 24.62 | 8.51 | 13.55 | 7.90 | 6.32 | 3.34 | 2.53 | 2.61 | 4.89 |\\n| FMoW-WILDS | Natural | 5.53 | 4.31 | 33.53 | 12.84 | 5.94 | 4.45 | 5.74 | 3.06 | 2.70 | 3.02 | 2.72 |\\n| RxRx1-WILDS | Natural | 5.80 | 5.72 | 7.90 | 4.84 | 5.98 | 5.98 | 6.03 | 4.66 | 4.56 | 4.41 | 4.47 |\\n| Amazon-WILDS | Natural | 2.40 | 2.29 | 8.01 | 2.38 | 2.40 | 2.28 | 17.87 | 1.65 | 1.62 | 1.60 | 1.50 |\\n| CivilCom.-WILDS | Natural | 12.64 | 10.80 | 16.76 | 11.03 | 13.31 | 10.99 | 16.65 | 7.14 |  |  |  |\\n| MNIST | Natural | 18.48 | 15.99 | 21.17 | 14.81 | 20.19 | 14.56 | 24.42 | 5.02 | 2.40 | 3.14 | 3.50 |\\n| EntitY-13 | Same | 16.23 | 11.14 | 24.97 | 10.88 | 19.08 | 10.47 | 10.71 | 5.39 | 3.88 | 4.58 | 4.19 |\\n|  | Novel | 28.53 | 22.02 | 38.33 | 21.64 | 32.43 | 21.22 | 20.61 | 13.58 | 10.28 | 12.25 | 6.63 |\\n| EntitY-30 | Same | 18.59 | 14.46 | 28.82 | 14.30 | 21.63 | 13.46 | 12.92 | 9.12 | 7.75 | 8.15 | 7.64 |\\n|  | Novel | 32.34 | 26.85 | 44.02 | 26.27 | 36.82 | 25.42 | 23.16 | 17.75 | 14.30 | 15.60 | 10.57 |\\n| NONLIVING-26 | Same | 18.66 | 17.17 | 26.39 | 16.14 | 19.86 | 15.58 | 16.63 | 10.87 | 10.24 | 10.07 | 10.26 |\\n|  | Novel | 33.43 | 31.53 | 41.66 | 29.87 | 35.13 | 29.31 | 29.56 | 21.70 | 20.12 | 19.08 | 18.26 |\\n| LIVING-17 | Same | 12.63 | 11.05 | 18.32 | 10.46 | 14.43 | 10.14 | 9.87 | 4.57 | 3.95 | 3.81 | 4.21 |\\n|  | Novel | 29.03 | 26.96 | 35.67 | 26.11 | 31.73 | 25.73 | 23.53 | 16.15 | 14.49 | 12.97 | 11.39 |\",\n",
      "        \"Table 1: Mean Absolute estimation Error (MAE) results for different datasets in our setup grouped by the nature of shift. 'Same' refers to same subpopulation shifts and 'Novel' refers novel subpopulation shifts. We include details about the target sets considered in each shift in Table 2. Post T denotes use of TS calibration on source. Across all datasets, we observe that ATC achieves superior performance (lower MAE is better). For language datasets, we use DistilBERT-base-uncased, for vision dataset we report results with DenseNet model with the exception of MNIST where we use FCN. We include results on other architectures in App. H. For GDE post T and pre T estimates match since TS doesn\\u2019t alter the argmax prediction. Results reported by aggregating MAE numbers over 4 different seeds. We include results with standard deviation values in Table 3.\",\n",
      "        \"Average Confidence (AC). Error is estimated as the expected value of the maximum softmax confidence on the target data, i.e, $\\\\mathrm{AC}_{\\\\mathcal{D}^{\\\\dagger}}=\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}^{\\\\dagger}}\\\\left[\\\\max _{j \\\\in \\\\mathcal{Y}} f_{j}(x)\\\\right]$.\\nDifference Of Confidence (DOC). We estimate error on target by subtracting difference of confidences on source and target (as a surrogate to distributional distance Guillory et al. (2021)) from the error on source distribution, i.e, $\\\\mathrm{DOC}_{\\\\mathcal{D}^{\\\\dagger}}=\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}^{\\\\delta}}[\\\\mathbb{I}\\\\left[\\\\arg \\\\max _{j \\\\in \\\\mathcal{Y}} f_{j}(x) \\\\neq y\\\\right]]+\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}^{\\\\dagger}}\\\\left[\\\\max _{j \\\\in \\\\mathcal{Y}} f_{j}(x)\\\\right]-$ $\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}^{\\\\delta}}\\\\left[\\\\max _{j \\\\in \\\\mathcal{Y}} f_{j}(x)\\\\right]$. This is referred to as DOC-Feat in (Guillory et al., 2021).\\nImportance re-weighting (IM). We estimate the error of the classifier with importance re-weighting of $0-1$ error in the pushforward space of the classifier. This corresponds to MANDOLIN using one slice based on the underlying classifier confidence Chen et al. (2021b).\",\n",
      "        \"Generalized Disagreement Equality (GDE). Error is estimated as the expected disagreement of two models (trained on the same training set but with different randomization) on target data (Jiang et al., 2021), i.e., $\\\\operatorname{GDE}_{\\\\mathcal{D}^{\\\\dagger}}=\\\\mathbb{E}_{x \\\\sim \\\\mathcal{D}^{\\\\dagger}}\\\\left[\\\\mathbb{I}\\\\left[f(x) \\\\neq f^{\\\\prime}(x)\\\\right]\\\\right]$ where $f$ and $f^{\\\\prime}$ are the two models. Note that GDE requires two models trained independently, doubling the computational overhead while training.\",\n",
      "        \"In Table 1, we report MAE results aggregated by the nature of the shift in our testbed. In Fig. 2 and Fig. 1(right), we show scatter plots for predicted accuracy versus OOD accuracy on several datasets. We include scatter plots for all datasets and parallel results with other architectures in App. H. In App. H.1, we also perform ablations on CIFAR using a pre-trained model and observe that pre-training doesn't change the efficacy of ATC.\"\n",
      "    ],\n",
      "    \"lists\": [],\n",
      "    \"links\": [],\n",
      "    \"images\": [],\n",
      "    \"tables\": [\n",
      "        {\n",
      "            \"headers\": [\n",
      "                \" Dataset \",\n",
      "                \" Shift \",\n",
      "                \" IM \",\n",
      "                \"  \",\n",
      "                \" AC \",\n",
      "                \"  \",\n",
      "                \" DOC \",\n",
      "                \"  \",\n",
      "                \" GDE \",\n",
      "                \" ATC-MC (Ours) \",\n",
      "                \"  \",\n",
      "                \" ATC-NE (Ours) \",\n",
      "                \"  \"\n",
      "            ],\n",
      "            \"rows\": [\n",
      "                [\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \",\n",
      "                    \" :--: \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \"  \",\n",
      "                    \" Pre T \",\n",
      "                    \" Post T \",\n",
      "                    \" Pre T \",\n",
      "                    \" Post T \",\n",
      "                    \" Pre T \",\n",
      "                    \" Post T \",\n",
      "                    \" Post T \",\n",
      "                    \" Pre T \",\n",
      "                    \" Post T \",\n",
      "                    \" Pre T \",\n",
      "                    \" Post T \"\n",
      "                ],\n",
      "                [\n",
      "                    \" CIFAR10 \",\n",
      "                    \" Natural \",\n",
      "                    \" 6.60 \",\n",
      "                    \" 5.74 \",\n",
      "                    \" 9.88 \",\n",
      "                    \" 6.89 \",\n",
      "                    \" 7.25 \",\n",
      "                    \" 6.07 \",\n",
      "                    \" 4.77 \",\n",
      "                    \" 3.21 \",\n",
      "                    \" 3.02 \",\n",
      "                    \" 2.99 \",\n",
      "                    \" 2.85 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Synthetic \",\n",
      "                    \" 12.33 \",\n",
      "                    \" 10.20 \",\n",
      "                    \" 16.50 \",\n",
      "                    \" 11.91 \",\n",
      "                    \" 13.87 \",\n",
      "                    \" 11.08 \",\n",
      "                    \" 6.55 \",\n",
      "                    \" 4.65 \",\n",
      "                    \" 4.25 \",\n",
      "                    \" 4.21 \",\n",
      "                    \" 3.87 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" CIFAR100 \",\n",
      "                    \" Synthetic \",\n",
      "                    \" 13.69 \",\n",
      "                    \" 11.51 \",\n",
      "                    \" 23.61 \",\n",
      "                    \" 13.10 \",\n",
      "                    \" 14.60 \",\n",
      "                    \" 10.14 \",\n",
      "                    \" 9.85 \",\n",
      "                    \" 5.50 \",\n",
      "                    \" 4.75 \",\n",
      "                    \" 4.72 \",\n",
      "                    \" 4.94 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" ImageNet200 \",\n",
      "                    \" Natural \",\n",
      "                    \" 12.37 \",\n",
      "                    \" 8.19 \",\n",
      "                    \" 22.07 \",\n",
      "                    \" 8.61 \",\n",
      "                    \" 15.17 \",\n",
      "                    \" 7.81 \",\n",
      "                    \" 5.13 \",\n",
      "                    \" 4.37 \",\n",
      "                    \" 2.04 \",\n",
      "                    \" 3.79 \",\n",
      "                    \" 1.45 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Synthetic \",\n",
      "                    \" 19.86 \",\n",
      "                    \" 12.94 \",\n",
      "                    \" 32.44 \",\n",
      "                    \" 13.35 \",\n",
      "                    \" 25.02 \",\n",
      "                    \" 12.38 \",\n",
      "                    \" 5.41 \",\n",
      "                    \" 5.93 \",\n",
      "                    \" 3.09 \",\n",
      "                    \" 5.00 \",\n",
      "                    \" 2.68 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" ImageNet \",\n",
      "                    \" Natural \",\n",
      "                    \" 7.77 \",\n",
      "                    \" 6.50 \",\n",
      "                    \" 18.13 \",\n",
      "                    \" 6.02 \",\n",
      "                    \" 8.13 \",\n",
      "                    \" 5.76 \",\n",
      "                    \" 6.23 \",\n",
      "                    \" 3.88 \",\n",
      "                    \" 2.17 \",\n",
      "                    \" 2.06 \",\n",
      "                    \" 0.80 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Synthetic \",\n",
      "                    \" 13.39 \",\n",
      "                    \" 10.12 \",\n",
      "                    \" 24.62 \",\n",
      "                    \" 8.51 \",\n",
      "                    \" 13.55 \",\n",
      "                    \" 7.90 \",\n",
      "                    \" 6.32 \",\n",
      "                    \" 3.34 \",\n",
      "                    \" 2.53 \",\n",
      "                    \" 2.61 \",\n",
      "                    \" 4.89 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" FMoW-WILDS \",\n",
      "                    \" Natural \",\n",
      "                    \" 5.53 \",\n",
      "                    \" 4.31 \",\n",
      "                    \" 33.53 \",\n",
      "                    \" 12.84 \",\n",
      "                    \" 5.94 \",\n",
      "                    \" 4.45 \",\n",
      "                    \" 5.74 \",\n",
      "                    \" 3.06 \",\n",
      "                    \" 2.70 \",\n",
      "                    \" 3.02 \",\n",
      "                    \" 2.72 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" RxRx1-WILDS \",\n",
      "                    \" Natural \",\n",
      "                    \" 5.80 \",\n",
      "                    \" 5.72 \",\n",
      "                    \" 7.90 \",\n",
      "                    \" 4.84 \",\n",
      "                    \" 5.98 \",\n",
      "                    \" 5.98 \",\n",
      "                    \" 6.03 \",\n",
      "                    \" 4.66 \",\n",
      "                    \" 4.56 \",\n",
      "                    \" 4.41 \",\n",
      "                    \" 4.47 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" Amazon-WILDS \",\n",
      "                    \" Natural \",\n",
      "                    \" 2.40 \",\n",
      "                    \" 2.29 \",\n",
      "                    \" 8.01 \",\n",
      "                    \" 2.38 \",\n",
      "                    \" 2.40 \",\n",
      "                    \" 2.28 \",\n",
      "                    \" 17.87 \",\n",
      "                    \" 1.65 \",\n",
      "                    \" 1.62 \",\n",
      "                    \" 1.60 \",\n",
      "                    \" 1.50 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" CivilCom.-WILDS \",\n",
      "                    \" Natural \",\n",
      "                    \" 12.64 \",\n",
      "                    \" 10.80 \",\n",
      "                    \" 16.76 \",\n",
      "                    \" 11.03 \",\n",
      "                    \" 13.31 \",\n",
      "                    \" 10.99 \",\n",
      "                    \" 16.65 \",\n",
      "                    \" 7.14 \",\n",
      "                    \"  \",\n",
      "                    \"  \",\n",
      "                    \"  \"\n",
      "                ],\n",
      "                [\n",
      "                    \" MNIST \",\n",
      "                    \" Natural \",\n",
      "                    \" 18.48 \",\n",
      "                    \" 15.99 \",\n",
      "                    \" 21.17 \",\n",
      "                    \" 14.81 \",\n",
      "                    \" 20.19 \",\n",
      "                    \" 14.56 \",\n",
      "                    \" 24.42 \",\n",
      "                    \" 5.02 \",\n",
      "                    \" 2.40 \",\n",
      "                    \" 3.14 \",\n",
      "                    \" 3.50 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" EntitY-13 \",\n",
      "                    \" Same \",\n",
      "                    \" 16.23 \",\n",
      "                    \" 11.14 \",\n",
      "                    \" 24.97 \",\n",
      "                    \" 10.88 \",\n",
      "                    \" 19.08 \",\n",
      "                    \" 10.47 \",\n",
      "                    \" 10.71 \",\n",
      "                    \" 5.39 \",\n",
      "                    \" 3.88 \",\n",
      "                    \" 4.58 \",\n",
      "                    \" 4.19 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Novel \",\n",
      "                    \" 28.53 \",\n",
      "                    \" 22.02 \",\n",
      "                    \" 38.33 \",\n",
      "                    \" 21.64 \",\n",
      "                    \" 32.43 \",\n",
      "                    \" 21.22 \",\n",
      "                    \" 20.61 \",\n",
      "                    \" 13.58 \",\n",
      "                    \" 10.28 \",\n",
      "                    \" 12.25 \",\n",
      "                    \" 6.63 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" EntitY-30 \",\n",
      "                    \" Same \",\n",
      "                    \" 18.59 \",\n",
      "                    \" 14.46 \",\n",
      "                    \" 28.82 \",\n",
      "                    \" 14.30 \",\n",
      "                    \" 21.63 \",\n",
      "                    \" 13.46 \",\n",
      "                    \" 12.92 \",\n",
      "                    \" 9.12 \",\n",
      "                    \" 7.75 \",\n",
      "                    \" 8.15 \",\n",
      "                    \" 7.64 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Novel \",\n",
      "                    \" 32.34 \",\n",
      "                    \" 26.85 \",\n",
      "                    \" 44.02 \",\n",
      "                    \" 26.27 \",\n",
      "                    \" 36.82 \",\n",
      "                    \" 25.42 \",\n",
      "                    \" 23.16 \",\n",
      "                    \" 17.75 \",\n",
      "                    \" 14.30 \",\n",
      "                    \" 15.60 \",\n",
      "                    \" 10.57 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" NONLIVING-26 \",\n",
      "                    \" Same \",\n",
      "                    \" 18.66 \",\n",
      "                    \" 17.17 \",\n",
      "                    \" 26.39 \",\n",
      "                    \" 16.14 \",\n",
      "                    \" 19.86 \",\n",
      "                    \" 15.58 \",\n",
      "                    \" 16.63 \",\n",
      "                    \" 10.87 \",\n",
      "                    \" 10.24 \",\n",
      "                    \" 10.07 \",\n",
      "                    \" 10.26 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Novel \",\n",
      "                    \" 33.43 \",\n",
      "                    \" 31.53 \",\n",
      "                    \" 41.66 \",\n",
      "                    \" 29.87 \",\n",
      "                    \" 35.13 \",\n",
      "                    \" 29.31 \",\n",
      "                    \" 29.56 \",\n",
      "                    \" 21.70 \",\n",
      "                    \" 20.12 \",\n",
      "                    \" 19.08 \",\n",
      "                    \" 18.26 \"\n",
      "                ],\n",
      "                [\n",
      "                    \" LIVING-17 \",\n",
      "                    \" Same \",\n",
      "                    \" 12.63 \",\n",
      "                    \" 11.05 \",\n",
      "                    \" 18.32 \",\n",
      "                    \" 10.46 \",\n",
      "                    \" 14.43 \",\n",
      "                    \" 10.14 \",\n",
      "                    \" 9.87 \",\n",
      "                    \" 4.57 \",\n",
      "                    \" 3.95 \",\n",
      "                    \" 3.81 \",\n",
      "                    \" 4.21 \"\n",
      "                ],\n",
      "                [\n",
      "                    \"  \",\n",
      "                    \" Novel \",\n",
      "                    \" 29.03 \",\n",
      "                    \" 26.96 \",\n",
      "                    \" 35.67 \",\n",
      "                    \" 26.11 \",\n",
      "                    \" 31.73 \",\n",
      "                    \" 25.73 \",\n",
      "                    \" 23.53 \",\n",
      "                    \" 16.15 \",\n",
      "                    \" 14.49 \",\n",
      "                    \" 12.97 \",\n",
      "                    \" 11.39 \"\n",
      "                ]\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"code_blocks\": [],\n",
      "    \"blockquotes\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "class MarkdownToJsonConverter:\n",
    "    def __init__(self, markdown_text):\n",
    "        self.text = markdown_text\n",
    "        self.json_data = {\n",
    "            \"title\": None,\n",
    "            \"headers\": [],\n",
    "            \"paragraphs\": [],\n",
    "            \"lists\": [],\n",
    "            \"links\": [],\n",
    "            \"images\": [],\n",
    "            \"tables\": [],\n",
    "            \"code_blocks\": [],\n",
    "            \"blockquotes\": []\n",
    "        }\n",
    "\n",
    "    def parse_headers(self):\n",
    "        headers = re.findall(r\"^(#{1,6})\\s+(.*)\", self.text, re.MULTILINE)\n",
    "        for header in headers:\n",
    "            self.json_data[\"headers\"].append({\n",
    "                \"level\": len(header[0]),\n",
    "                \"text\": header[1]\n",
    "            })\n",
    "        if headers and len(headers[0][0]) == 1:\n",
    "            self.json_data[\"title\"] = headers[0][1]\n",
    "\n",
    "    def parse_paragraphs(self):\n",
    "        paragraphs = re.split(r\"\\n\\n+\", self.text)\n",
    "        for para in paragraphs:\n",
    "            if not para.startswith(\"#\") and not para.startswith(\"-\") and not re.search(r\"\\[(.*?)\\]\\((.*?)\\)\", para):\n",
    "                self.json_data[\"paragraphs\"].append(para.strip())\n",
    "\n",
    "    def parse_lists(self):\n",
    "        lists = re.findall(r\"(?m)^[-*+]\\s+(.*)\", self.text)\n",
    "        if lists:\n",
    "            self.json_data[\"lists\"].append(lists)\n",
    "\n",
    "    def parse_links(self):\n",
    "        links = re.findall(r\"\\[(.*?)\\]\\((.*?)\\)\", self.text)\n",
    "        for link in links:\n",
    "            self.json_data[\"links\"].append({\n",
    "                \"text\": link[0],\n",
    "                \"url\": link[1]\n",
    "            })\n",
    "\n",
    "    def parse_images(self):\n",
    "        images = re.findall(r\"!\\[(.*?)\\]\\((.*?)\\)\", self.text)\n",
    "        for image in images:\n",
    "            self.json_data[\"images\"].append({\n",
    "                \"alt_text\": image[0],\n",
    "                \"url\": image[1]\n",
    "            })\n",
    "\n",
    "    def parse_tables(self):\n",
    "        tables = re.findall(r\"(?:\\|.+?\\|\\n)+\", self.text)\n",
    "        for table in tables:\n",
    "            rows = [row.strip().split(\"|\")[1:-1] for row in table.split(\"\\n\") if row.startswith(\"|\")]\n",
    "            if rows:\n",
    "                headers = rows[0]\n",
    "                data_rows = rows[1:]\n",
    "                self.json_data[\"tables\"].append({\n",
    "                    \"headers\": headers,\n",
    "                    \"rows\": data_rows\n",
    "                })\n",
    "\n",
    "    def parse_code_blocks(self):\n",
    "        code_blocks = re.findall(r\"```(.*?)\\n(.*?)```\", self.text, re.S)\n",
    "        for block in code_blocks:\n",
    "            self.json_data[\"code_blocks\"].append({\n",
    "                \"language\": block[0].strip(),\n",
    "                \"code\": block[1].strip()\n",
    "            })\n",
    "\n",
    "    def parse_blockquotes(self):\n",
    "        blockquotes = re.findall(r\"^>\\s+(.*)\", self.text, re.MULTILINE)\n",
    "        for quote in blockquotes:\n",
    "            self.json_data[\"blockquotes\"].append(quote.strip())\n",
    "\n",
    "    def convert(self):\n",
    "        self.parse_headers()\n",
    "        self.parse_paragraphs()\n",
    "        self.parse_lists()\n",
    "        self.parse_links()\n",
    "        self.parse_images()\n",
    "        self.parse_tables()\n",
    "        self.parse_code_blocks()\n",
    "        self.parse_blockquotes()\n",
    "        return json.dumps(self.json_data, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "converter = MarkdownToJsonConverter(md[6])\n",
    "print(converter.convert())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph(G):\n",
    "    # Visualize the graph using matplotlib\n",
    "    pos = nx.spring_layout(G, seed=42)  # Layout for visualization\n",
    "    plt.figure(figsize=(10, 10))  # Increase figure size for better visualization\n",
    "    nx.draw(G, pos, with_labels=True, node_size=5000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", arrows=True)\n",
    "    plt.title(\"Markdown Structure Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "class MarkdownParser:\n",
    "    def __init__(self, markdown_text):\n",
    "        self.text = markdown_text\n",
    "        self.graph = nx.DiGraph()  # Directed graph to show hierarchy and relationships\n",
    "        self.current_node = None\n",
    "        self.json_data = {\n",
    "            \"title\": None,\n",
    "            \"headers\": [],\n",
    "            \"paragraphs\": [],\n",
    "            \"lists\": [],\n",
    "            \"links\": [],\n",
    "            \"images\": [],\n",
    "            \"tables\": [],\n",
    "            \"code_blocks\": [],\n",
    "            \"blockquotes\": []\n",
    "        }\n",
    "\n",
    "    # Add node to the graph and JSON structure\n",
    "    def add_node(self, node_type, content, parent=None):\n",
    "        node_id = f\"{node_type}_{len(self.graph.nodes)}\"\n",
    "        self.graph.add_node(node_id, type=node_type, content=content)\n",
    "        if parent:\n",
    "            self.graph.add_edge(parent, node_id)\n",
    "        # Add to JSON structure as well\n",
    "        if node_type == \"Page\":\n",
    "            self.json_data[\"title\"] = content.get(\"title\")\n",
    "        elif node_type.startswith(\"Header\"):\n",
    "            self.json_data[\"headers\"].append(content)\n",
    "        elif node_type == \"List\":\n",
    "            self.json_data[\"lists\"].append(content)\n",
    "        elif node_type == \"Link\":\n",
    "            self.json_data[\"links\"].append(content)\n",
    "        elif node_type == \"Image\":\n",
    "            self.json_data[\"images\"].append(content)\n",
    "        elif node_type == \"CodeBlock\":\n",
    "            self.json_data[\"code_blocks\"].append(content)\n",
    "        elif node_type == \"Blockquote\":\n",
    "            self.json_data[\"blockquotes\"].append(content)\n",
    "        return node_id\n",
    "\n",
    "    def parse_headers(self):\n",
    "        headers = re.findall(r\"^(#{1,6})\\s+(.*)\", self.text, re.MULTILINE)\n",
    "        for header in headers:\n",
    "            level = len(header[0])\n",
    "            node_id = self.add_node(f\"Header-{level}\", {\"text\": header[1]}, self.current_node)\n",
    "            self.current_node = node_id  # Update current node to this header\n",
    "            \n",
    "            self.json_data[\"headers\"].append({\n",
    "                \"level\": len(header[0]),\n",
    "                \"text\": header[1]\n",
    "            })\n",
    "            \n",
    "        if headers and len(headers[0][0]) == 1:\n",
    "            self.json_data[\"title\"] = headers[0][1]\n",
    "\n",
    "    def parse_lists(self):\n",
    "        lists = re.findall(r\"(?m)^[-*+]\\s+(.*)\", self.text)\n",
    "        if lists:\n",
    "            list_node_id = self.add_node(\"List\", {\"items\": lists}, self.current_node)\n",
    "            for item in lists:\n",
    "                self.add_node(\"ListItem\", {\"text\": item}, list_node_id)\n",
    "\n",
    "    def parse_links(self):\n",
    "        links = re.findall(r\"\\[(.*?)\\]\\((.*?)\\)\", self.text)\n",
    "        for link in links:\n",
    "            self.add_node(\"Link\", {\"text\": link[0], \"url\": link[1]}, self.current_node)\n",
    "\n",
    "    def parse_images(self):\n",
    "        images = re.findall(r\"!\\[(.*?)\\]\\((.*?)\\)\", self.text)\n",
    "        for image in images:\n",
    "            self.add_node(\"Image\", {\"alt_text\": image[0], \"url\": image[1]}, self.current_node)\n",
    "\n",
    "    def parse_blockquotes(self):\n",
    "        blockquotes = re.findall(r\"^>\\s+(.*)\", self.text, re.MULTILINE)\n",
    "        for quote in blockquotes:\n",
    "            self.add_node(\"Blockquote\", {\"text\": quote}, self.current_node)\n",
    "\n",
    "    def parse_code_blocks(self):\n",
    "        code_blocks = re.findall(r\"```(.*?)\\n(.*?)```\", self.text, re.S)\n",
    "        for block in code_blocks:\n",
    "            self.add_node(\"CodeBlock\", {\"language\": block[0].strip(), \"code\": block[1].strip()}, self.current_node)\n",
    "\n",
    "    def parse(self):\n",
    "        self.add_node(\"Page\", {\"title\": \"Markdown Document\"})  # Root node representing the page\n",
    "        self.parse_headers()\n",
    "        self.parse_lists()\n",
    "        self.parse_links()\n",
    "        self.parse_images()\n",
    "        self.parse_blockquotes()\n",
    "        self.parse_code_blocks()\n",
    "        return self.graph\n",
    "\n",
    "    def get_json(self):\n",
    "        return json.dumps(self.json_data, indent=4)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "converter = MarkdownParser(md[6])\n",
    "g = converter.parse()\n",
    "visualize_graph(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: # LEVERAGING UNLABELED DATA TO PREDICT OUT-OF-DISTRIBUTION PERFORMANCE \n",
      "Content:\n",
      "Saurabh Garg*<br>Carnegie Mellon University<br>sgarg2@andrew.cmu.edu<br>Sivaraman Balakrishnan<br>Carnegie Mellon University<br>sbalakri@andrew.cmu.edu<br>Zachary C. Lipton<br>Carnegie Mellon University<br>zlipton@andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "Header: ## Behnam Neyshabur\n",
      "Content:\n",
      "Google Research, Blueshift team\n",
      "neyshabur@google.com\n",
      "\n",
      "Hanie Sedghi<br>Google Research, Brain team<br>hsedghi@google.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Header: #### Abstract\n",
      "Content:\n",
      "Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance $2-4 \\times$ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works ${ }^{1}$.\n",
      "\n",
      "\n",
      "## 1 INTRODUCTION\n",
      "\n",
      "Machine learning models deployed in the real world typically encounter examples from previously unseen distributions. While the IID assumption enables us to evaluate models using held-out data from the source distribution (from which training data is sampled), this estimate is no longer valid in presence of a distribution shift. Moreover, under such shifts, model accuracy tends to degrade (Szegedy et al., 2014; Recht et al., 2019; Koh et al., 2021). Commonly, the only data available to the practitioner are a labeled training set (source) and unlabeled deployment-time data which makes the problem more difficult. In this setting, detecting shifts in the distribution of covariates is known to be possible (but difficult) in theory (Ramdas et al., 2015), and in practice (Rabanser et al., 2018). However, producing an optimal predictor using only labeled source and unlabeled target data is well-known to be impossible absent further assumptions (Ben-David et al., 2010; Lipton et al., 2018).\n",
      "\n",
      "Two vital questions that remain are: (i) the precise conditions under which we can estimate a classifier's target-domain accuracy; and (ii) which methods are most practically useful. To begin, the straightforward way to assess the performance of a model under distribution shift would be to collect labeled (target domain) examples and then to evaluate the model on that data. However, collecting fresh labeled data from the target distribution is prohibitively expensive and time-consuming, especially if the target distribution is non-stationary. Hence, instead of using labeled data, we aim to use unlabeled data from the target distribution, that is comparatively abundant, to predict model performance. Note that in this work, our focus is not to improve performance on the target but, rather, to estimate the accuracy on the target for a given classifier.\n",
      "\n",
      "[^0]\n",
      "[^0]:    * Work done in part while Saurabh Garg was interning at Google\n",
      "    ${ }^{1}$ Code is available at https://github.com/saurabhgarg1996/ATC_code.\n",
      "\n",
      "Header: # 2 PRIOR WORK \n",
      "Content:\n",
      "Out-of-distribution detection. The main goal of OOD detection is to identify previously unseen examples, i.e., samples out of the support of training distribution. To accomplish this, modern methods utilize confidence or features learned by a deep network trained on some source data. Hendrycks \\& Gimpel (2016); Geifman \\& El-Yaniv (2017) used the confidence score of an (already) trained deep model to identify OOD points. Lakshminarayanan et al. (2016) use entropy of an ensemble model to evaluate prediction uncertainty on OOD points. To improve OOD detection with model confidence, Liang et al. (2017) propose to use temperature scaling and input perturbations. Jiang et al. (2018) propose to use scores based on the relative distance of the predicted class to the second class. Recently, residual flow-based methods were used to obtain a density model for OOD detection (Zhang et al., 2020). Ji et al. (2021) proposed a method based on subfunction error bounds to compute unreliability per sample. Refer to Ovadia et al. (2019); Ji et al. (2021) for an overview and comparison of methods for prediction uncertainty on OOD data.\n",
      "\n",
      "Predicting model generalization. Understanding generalization capabilities of overparameterized models on in-distribution data using conventional machine learning tools has been a focus of a long line of work; representative research includes Neyshabur et al. (2015; 2017); Neyshabur (2017); Neyshabur et al. (2018); Dziugaite \\& Roy (2017); Bartlett et al. (2017); Zhou et al. (2018); Long \\& Sedghi (2019); Nagarajan \\& Kolter (2019a). At a high level, this line of research bounds the generalization gap directly with complexity measures calculated on the trained model. However, these bounds typically remain numerically loose relative to the true generalization error (Zhang et al., 2016; Nagarajan \\& Kolter, 2019b). On the other hand, another line of research departs from complexitybased approaches to use unseen unlabeled data to predict in-distribution generalization (Platanios et al., 2016; 2017; Garg et al., 2021; Jiang et al., 2021).\n",
      "\n",
      "Relevant to our work are methods for predicting the error of a classifier on OOD data based on unlabeled data from the target (OOD) domain. These methods can be characterized into two broad categories: (i) Methods which explicitly predict correctness of the model on individual unlabeled points (Deng \\& Zheng, 2021; Jiang et al., 2021; Deng et al., 2021; Chen et al., 2021a); and (ii) Methods which directly obtain an estimate of error with unlabeled OOD data without making a point-wise prediction (Chen et al., 2021b; Guillory et al., 2021; Chuang et al., 2020).\n",
      "To achieve a consistent estimate of the target accuracy, Jiang et al. (2021); Guillory et al. (2021) require calibration on target domain. However, these methods typically yield poor estimates as deep models trained and calibrated on some source data are seldom calibrated on previously unseen domains (Ovadia et al., 2019). Additionally, Deng \\& Zheng (2021); Guillory et al. (2021) derive model-based distribution statistics on unlabeled target set that correlate with the target accuracy and propose to use a subset of labeled target domains to learn a (linear) regression function that predicts model performance. However, there are two drawbacks with this approach: (i) the correlation of these distribution statistics can vary substantially as we consider different nature of shifts (refer to Sec. 5.1, where we empirically demonstrate this failure); (ii) even if there exists a (hypothetical) statistic with strong correlations, obtaining labeled target domains (even simulated ones) with strong correlations would require significant a priori knowledge about the nature of shift that, in general, might not be available before models are deployed in the wild. Nonetheless, in our work, we only assume access to labeled data from the source domain presuming no access to labeled target domains or information about how to simulate them.\n",
      "\n",
      "Interactive graph saved as document_graph.html. Open this file in your browser for an explorable visualization.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "class MarkdownGraph:\n",
    "    def __init__(self, pages, file_name):\n",
    "        \"\"\"Initialize with a list of pages.\"\"\"\n",
    "        self.pages = pages\n",
    "        self.graph = nx.Graph()\n",
    "        self.document_node = file_name\n",
    "        self.graph.add_node(self.document_node, type=\"Document\", name='file_name', content=\"\")\n",
    "\n",
    "        self.headers_count = 0 \n",
    "        self.title_count = 0\n",
    "\n",
    "        self.parse_pages()\n",
    "\n",
    "\n",
    "    def parse_pages(self):\n",
    "        \"\"\"Parse the pages and create nodes for document, pages, and entities.\"\"\"\n",
    "        # Iterate through each page\n",
    "        for page_idx, self.page_text in enumerate(self.pages, start=1):\n",
    "\n",
    "            self.page_node = f\"page_{page_idx}\"\n",
    "            self.graph.add_node(self.page_node, type=\"Page\", content=\"\")\n",
    "            self.graph.add_edge(self.document_node, self.page_node)  # Connect each page to the document\n",
    "\n",
    "\n",
    "            self.parse_headers()\n",
    "            self.parse_paragraphs()\n",
    "\n",
    "            if page_idx > 2:\n",
    "                break\n",
    "\n",
    "    def parse_headers(self):\n",
    "        headers = re.findall(r\"^(#{1,6})\\s+(.*)\", self.page_text, re.MULTILINE)\n",
    "        for header in headers:\n",
    "\n",
    "            hi = self.headers_count\n",
    "            header_node = f\"title_{hi}\"\n",
    "            self.graph.add_node(\n",
    "                header_node, \n",
    "                type=\"Header\", \n",
    "                content=header[1], \n",
    "                level=len(header[0])\n",
    "            )\n",
    "            self.graph.add_edge(self.page_node, header_node)\n",
    "            self.headers_count += 1\n",
    "\n",
    "        if headers and len(headers[0][0]) == 1:\n",
    "            ti = self.title_count\n",
    "            title_node = f\"title_{ti}\"\n",
    "            self.graph.add_node(\n",
    "                title_node, \n",
    "                type=\"Title\", \n",
    "                content=headers[0][1], \n",
    "            )\n",
    "            self.graph.add_edge(self.page_node, title_node)\n",
    "            self.title_count += 1\n",
    "\n",
    "    def parse_paragraphs(self):\n",
    "        sections = re.findall(r\"(#{1,6})\\s*([^\\n]+)\\s*([\\s\\S]*?)(?=\\1\\s*[^\\n]+|\\Z)\", self.page_text)\n",
    "        for header, name, content in sections:\n",
    "            print(f\"Header: {header} {name}\\nContent:\\n{content}\\n\")\n",
    "        paragraphs = re.split(r\"(#{1,6})\\s*Header1\\s*([\\s\\S]*?)\\1\\s*Header2\", self.page_text)\n",
    "        for pi, para in enumerate(paragraphs):\n",
    "            if not para.startswith(\"#\") and not para.startswith(\"-\") and not re.search(r\"\\[(.*?)\\]\\((.*?)\\)\", para):\n",
    "\n",
    "                paragraph_node = f\"paragraph_{pi}\"\n",
    "                self.graph.add_node(\n",
    "                    paragraph_node, \n",
    "                    type=\"Paragraph\", \n",
    "                    content=para.strip(), \n",
    "                )\n",
    "                self.graph.add_edge(self.page_node, paragraph_node)\n",
    "\n",
    "\n",
    "    def parse_pages_off(self):\n",
    "        \"\"\"Parse the pages and create nodes for document, pages, and entities.\"\"\"\n",
    "        # Iterate through each page\n",
    "        for page_idx, page_text in enumerate(self.pages, start=1):\n",
    "            page_node = f\"Page {page_idx}\"\n",
    "            self.graph.add_node(page_node, type=\"Page\", content=f\"\")\n",
    "            self.graph.add_edge(self.document_node, page_node)  # Connect each page to the document\n",
    "\n",
    "            # Regular expressions for detecting title, subtitle, paragraphs, figures, and tables\n",
    "            title_pattern = r\"^# (.*)\"  # Match the title (first line starting with '#')\n",
    "            subtitle_pattern = r\"^## (.*)\"  # Match subtitles (lines starting with '##')\n",
    "            paragraph_pattern = r\"([^\\n]+)\\n\"  # Match paragraphs (non-empty lines)\n",
    "            figure_pattern = r\"!\\[.*\\]\\((.*\\.jpg|.*\\.png|.*\\.jpeg|.*\\.pdf)\\)\"  # Detect image references (figures)\n",
    "            table_pattern = r\"(Table \\d+)\"  # Detect table references\n",
    "\n",
    "            # Detect the title, subtitle, and paragraphs for this page\n",
    "            title = re.search(title_pattern, page_text)\n",
    "            subtitles = re.findall(subtitle_pattern, page_text)\n",
    "            paragraphs = re.findall(paragraph_pattern, page_text)\n",
    "            figures = re.findall(figure_pattern, page_text)\n",
    "            tables = re.findall(table_pattern, page_text)\n",
    "\n",
    "            # Add the title as a node and connect it to the page\n",
    "            if title:\n",
    "                title_node = f\"Title {page_idx}\"\n",
    "                self.graph.add_node(title_node, type=\"Title\", content=title.group(1))\n",
    "                self.graph.add_edge(page_node, title_node)\n",
    "\n",
    "            # Add subtitles as nodes and connect them to the page\n",
    "            for i, subtitle in enumerate(subtitles, start=1):\n",
    "                subtitle_node = f\"Subtitle {page_idx}-{i}\"\n",
    "                self.graph.add_node(subtitle_node, type=\"Subtitle\", content=subtitle)\n",
    "                self.graph.add_edge(page_node, subtitle_node)\n",
    "\n",
    "            # Add nodes for paragraphs and connect them to the page\n",
    "            self.add_paragraphs_and_references(page_node, paragraphs)\n",
    "\n",
    "            # Add figures and tables as nodes and connect them to the page and paragraphs\n",
    "            self.add_figures_and_tables(page_node, figures, tables)\n",
    "            if page_idx >3:\n",
    "\n",
    "                break\n",
    "\n",
    "    def add_paragraphs_and_references(self, page_node, paragraphs):\n",
    "        \"\"\"Add paragraph nodes and connect them to the page.\"\"\"\n",
    "        for i, paragraph in enumerate(paragraphs, start=1):\n",
    "            paragraph_node = f\"Paragraph {page_node}-{i}\"\n",
    "            self.graph.add_node(paragraph_node, type=\"Paragraph\", content=paragraph)\n",
    "            self.graph.add_edge(page_node, paragraph_node)  # Connect paragraph to the page\n",
    "            \n",
    "            # Check if the paragraph references any figures or tables (this will be handled by add_figures_and_tables)\n",
    "\n",
    "    def add_figures_and_tables(self, page_node, figures, tables):\n",
    "        \"\"\"Add figure and table nodes and connect them to the page and paragraphs.\"\"\"\n",
    "        for i, figure in enumerate(figures, start=1):\n",
    "            figure_node = f\"Figure {page_node}-{i}\"\n",
    "            self.graph.add_node(figure_node, type=\"Figure\", content=figure)\n",
    "            self.graph.add_edge(page_node, figure_node)  # Connect figure to the page\n",
    "            \n",
    "            # Find which paragraph references the figure and connect them\n",
    "            for j, paragraph in enumerate(self.pages, start=1):\n",
    "                if f\"Figure {i}\" in paragraph:\n",
    "                    self.graph.add_edge(f\"Paragraph {page_node}-{j}\", figure_node)\n",
    "\n",
    "        for i, table in enumerate(tables, start=1):\n",
    "            table_node = f\"Table {page_node}-{i}\"\n",
    "            self.graph.add_node(table_node, type=\"Table\", content=table)\n",
    "            self.graph.add_edge(page_node, table_node)  # Connect table to the page\n",
    "            \n",
    "            # Find which paragraph references the table and connect them\n",
    "            for j, paragraph in enumerate(self.pages, start=1):\n",
    "                if f\"Table {i}\" in paragraph:\n",
    "                    self.graph.add_edge(f\"Paragraph {page_node}-{j}\", table_node)\n",
    "\n",
    "    def visualize(self, output_filename=\"document_graph.html\"):\n",
    "        \"\"\"Generate and save an interactive graph using Pyvis.\"\"\"\n",
    "        # Initialize the Pyvis network object\n",
    "        net = Network(height=\"800px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"black\")\n",
    "        \n",
    "        # Convert the networkx graph to a Pyvis graph\n",
    "        net.from_nx(self.graph)\n",
    "\n",
    "        for node in self.graph.nodes:\n",
    "            content = self.graph.nodes[node]['content']\n",
    "            \n",
    "            if content:\n",
    "                net.get_node(node)['label'] = content\n",
    "\n",
    "        net.save_graph(output_filename)\n",
    "        \n",
    "        print(f\"Interactive graph saved as {output_filename}. Open this file in your browser for an explorable visualization.\")\n",
    "\n",
    "\n",
    "markdown_text = md\n",
    "graph_creator = MarkdownGraph(markdown_text, file_name)\n",
    "graph_creator.visualize(output_filename=\"document_graph.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
